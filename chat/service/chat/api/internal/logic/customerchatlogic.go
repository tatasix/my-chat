package logic

import (
	"chat/common/lock"
	"chat/common/util"
	"chat/common/wecom"
	"chat/service/chat/api/internal/repository"
	"chat/service/chat/api/internal/service"
	"chat/service/chat/api/internal/vars"
	"context"
	"crypto/md5"
	"encoding/json"
	"fmt"
	"io"
	"strings"
	"time"

	"chat/common/milvus"
	"chat/common/openai"
	"chat/common/redis"
	"chat/service/chat/api/internal/svc"
	"chat/service/chat/api/internal/types"
	"chat/service/chat/model"

	"github.com/zeromicro/go-zero/core/logx"
)

type CustomerChatLogic struct {
	logx.Logger
	ctx                   context.Context
	svcCtx                *svc.ServiceContext
	model                 string
	baseHost              string
	basePrompt            string
	message               string
	postModel             string
	wechatUserRepository  *repository.WechatUserRepository
	chatService           *service.ChatService
	configService         *service.ConfigService
	customerConfigService *service.CustomerConfigService
}

func NewCustomerChatLogic(ctx context.Context, svcCtx *svc.ServiceContext) *CustomerChatLogic {
	return &CustomerChatLogic{
		Logger:                logx.WithContext(ctx),
		ctx:                   ctx,
		svcCtx:                svcCtx,
		wechatUserRepository:  repository.NewWechatUserRepository(ctx, svcCtx),
		chatService:           service.NewChatService(ctx, svcCtx),
		configService:         service.NewConfigService(ctx, svcCtx),
		customerConfigService: service.NewCustomerConfigService(ctx, svcCtx),
	}
}

func (l *CustomerChatLogic) setModelName() (ls *CustomerChatLogic) {
	m := l.svcCtx.Config.WeCom.Model
	if m == "" {
		m = openai.TextModel
	}
	l.svcCtx.Config.WeCom.Model = m
	return l
}

func (l *CustomerChatLogic) setBasePrompt() (ls *CustomerChatLogic) {
	p := l.svcCtx.Config.WeCom.BasePrompt
	if p == "" {
		p = "ä½ æ˜¯ ChatGPT, ä¸€ä¸ªç”± OpenAI è®­ç»ƒçš„å¤§å‹è¯­è¨€æ¨¡å‹, ä½ æ—¨åœ¨å›ç­”å¹¶è§£å†³äººä»¬çš„ä»»ä½•é—®é¢˜ï¼Œå¹¶ä¸”å¯ä»¥ä½¿ç”¨å¤šç§è¯­è¨€ä¸äººäº¤æµã€‚\n"
	}
	l.basePrompt = p
	return l
}

func (l *CustomerChatLogic) setBaseHost() (ls *CustomerChatLogic) {
	if l.svcCtx.Config.OpenAi.Host == "" {
		l.svcCtx.Config.OpenAi.Host = "https://api.openai.com"
	}
	l.baseHost = l.svcCtx.Config.OpenAi.Host
	return l
}

func (l *CustomerChatLogic) FactoryCommend(req *types.CustomerChatReq) (proceed bool, err error) {
	template := make(map[string]CustomerTemplateData)
	//å½“ message ä»¥ # å¼€å¤´æ—¶ï¼Œè¡¨ç¤ºæ˜¯ç‰¹æ®ŠæŒ‡ä»¤
	if !strings.HasPrefix(req.Msg, "#") {
		return true, nil
	}

	template["#direct"] = CustomerCommendDirect{}
	template["#voice"] = CustomerCommendVoice{}
	template["#help"] = CustomerCommendHelp{}
	template["#system"] = CustomerCommendSystem{}
	template["#clear"] = CustomerCommendClear{}
	template["#about"] = CustomerCommendAbout{}

	for s, data := range template {
		if strings.HasPrefix(req.Msg, s) {
			proceed = data.customerExec(l, req)
			return proceed, nil
		}
	}

	return true, nil
}

type CustomerTemplateData interface {
	customerExec(svcCtx *CustomerChatLogic, req *types.CustomerChatReq) (proceed bool)
}

type CustomerCommendVoice struct{}

func (p CustomerCommendVoice) customerExec(l *CustomerChatLogic, req *types.CustomerChatReq) bool {
	msg := strings.Replace(req.Msg, "#voice:", "", -1)
	if msg == "" {
		sendToUser(req.OpenKfID, "", req.CustomerID, "ç³»ç»Ÿé”™è¯¯:æœªèƒ½è¯»å–åˆ°éŸ³é¢‘ä¿¡æ¯", l.svcCtx.Config)
		return false
	}
	openAiKey, err := l.configService.GetConfigKey()
	if err != nil {
		return false
	}
	c := openai.NewChatClient(l.ctx, openAiKey).
		WithModel(l.model).
		WithBaseHost(l.svcCtx.Config.OpenAi.Host).
		WithOrigin(l.svcCtx.Config.OpenAi.Origin).
		WithEngine(l.svcCtx.Config.OpenAi.Engine).
		WithPostModel(l.postModel)

	if l.svcCtx.Config.Proxy.Enable {
		c = c.WithHttpProxy(l.svcCtx.Config.Proxy.Http).WithSocks5Proxy(l.svcCtx.Config.Proxy.Socket5)
	}

	var cli openai.Speaker
	if l.svcCtx.Config.Speaker.Company == "" {
		l.svcCtx.Config.Speaker.Company = "openai"
	}
	switch l.svcCtx.Config.Speaker.Company {
	case "openai":
		logx.Info("ä½¿ç”¨openaiéŸ³é¢‘è½¬æ¢")
		cli = c
	case "ali":
		logx.Info("ä½¿ç”¨é˜¿é‡Œäº‘éŸ³é¢‘è½¬æ¢")
		//s, err := voice.NewSpeakClient(
		//	l.svcCtx.Config.Speaker.AliYun.AccessKeyId,
		//	l.svcCtx.Config.Speaker.AliYun.AccessKeySecret,
		//	l.svcCtx.Config.Speaker.AliYun.AppKey,
		//)
		//if err != nil {
		// sendToUser(req.OpenKfID, "", req.CustomerID, "é˜¿é‡Œäº‘éŸ³é¢‘è½¬æ¢åˆå§‹åŒ–å¤±è´¥:"+err.Error(), l.svcCtx.Config)
		//	return false
		//}
		//msg = strings.Replace(msg, ".mp3", ".amr", -1)
		//cli = s
	default:
		sendToUser(req.OpenKfID, "", req.CustomerID, "ç³»ç»Ÿé”™è¯¯:æœªçŸ¥çš„éŸ³é¢‘è½¬æ¢æœåŠ¡å•†", l.svcCtx.Config)
		return false
	}

	txt, err := cli.SpeakToTxt(msg)
	if txt == "" || err != nil {
		logx.Info("openaiè½¬æ¢é”™è¯¯", err.Error())
		sendToUser(req.OpenKfID, "", req.CustomerID, "ç³»ç»Ÿé”™è¯¯:éŸ³é¢‘ä¿¡æ¯è½¬æ¢é”™è¯¯", l.svcCtx.Config)
		return false
	}
	l.message = txt
	return true
}

type CustomerCommendClear struct{}

func (p CustomerCommendClear) customerExec(l *CustomerChatLogic, req *types.CustomerChatReq) bool {
	// æ¸…ç†ä¸Šä¸‹æ–‡
	openai.NewUserContext(l.ctx,
		openai.GetUserUniqueID(req.CustomerID, req.OpenKfID),
	).Clear()
	sendToUser(req.OpenKfID, "", req.CustomerID, "è®°å¿†æ¸…é™¤å®Œæˆ:æ¥å¼€å§‹æ–°ä¸€è½®çš„chatå§", l.svcCtx.Config)
	return false
}

func (l *CustomerChatLogic) CustomerChat(req *types.CustomerChatReq) (resp *types.CustomerChatReply, err error) {

	// åˆ›å»ºé”
	lockKey := fmt.Sprintf(lock.CustomerChat, "customer-chat", req.MsgID, req.CustomerID)
	res, err := lock.Lock(lockKey, time.Duration(240)*time.Second)
	if err != nil {
		return &types.CustomerChatReply{}, err
	}
	if !res {
		return &types.CustomerChatReply{}, fmt.Errorf("ç³»ç»Ÿç¹å¿™ï¼Œè¯·ç¨åå†è¯•ï½")
	}
	defer func() {
		_ = lock.Release(lockKey)
	}()

	l.setModelNameV2().setBasePrompt().setBaseHost()

	// å¹‚ç­‰
	exist, _, err := l.chatService.Idempotent(req.MsgID, req.CustomerID)
	if err != nil {
		return &types.CustomerChatReply{}, err
	}

	if exist != "" {
		wecom.SendCustomerChatMessage(req.OpenKfID, req.CustomerID, exist)
		return
	}
	questionModel := &model.ChatRecord{
		User:             req.CustomerID,
		MessageId:        req.MsgID,
		OpenKfId:         req.OpenKfID,
		Content:          req.Msg,
		ChatType:         repository.ChatTypeCustomer,
		AnswerOrQuestion: repository.AnswerOrQuestionQuestion,
		MessageType:      repository.MessageTypeText,
	}
	_ = l.chatService.Insert(questionModel)

	embeddingEnable := true
	embeddingMode := vars.EMBEDDING_MODEMBEDDING
	var baseScore float32
	var baseTopK int
	var clearContextTime int64
	var postModel string
	//get prompt
	//customerConfigPo, err := l.svcCtx.CustomerConfigModel.FindOneByQuery(l.ctx,
	//	l.svcCtx.CustomerConfigModel.RowBuilder().Where(squirrel.Eq{"kf_id": req.OpenKfID}),
	//)
	customerConfigPo, err := l.customerConfigService.GetPrompt(req.OpenKfID, req.CustomerID)

	if err != nil {
		return nil, err
	}
	if customerConfigPo != nil {
		if customerConfigPo.Prompt != "" {
			l.basePrompt = customerConfigPo.Prompt
		}
		embeddingEnable = customerConfigPo.EmbeddingEnable
		embeddingMode = customerConfigPo.EmbeddingMode
		if customerConfigPo.Score.Valid {
			baseScore = float32(customerConfigPo.Score.Float64)
		}
		if customerConfigPo.TopK != 0 {
			baseTopK = int(customerConfigPo.TopK)
		}
		clearContextTime = customerConfigPo.ClearContextTime
		postModel = customerConfigPo.PostModel
	}
	l.setPostModel(postModel)

	// æŒ‡ä»¤åŒ¹é…ï¼Œ æ ¹æ®å“åº”å€¼åˆ¤å®šæ˜¯å¦éœ€è¦å»è°ƒç”¨ openai æ¥å£äº†
	proceed, _ := l.FactoryCommend(req)
	if !proceed {
		return
	}
	if l.message != "" {
		req.Msg = l.message
	}
	openAiKey, err := l.configService.GetConfigKey()
	if err != nil {
		return
	}
	// openai client
	c := openai.NewChatClient(l.ctx, openAiKey).
		WithModel(l.model).
		WithBaseHost(l.baseHost).
		WithOrigin(l.svcCtx.Config.OpenAi.Origin).
		WithEngine(l.svcCtx.Config.OpenAi.Engine).WithPostModel(postModel)
	if l.svcCtx.Config.Proxy.Enable {
		c = c.WithHttpProxy(l.svcCtx.Config.Proxy.Http).WithSocks5Proxy(l.svcCtx.Config.Proxy.Socket5)
	}

	// context
	collection := openai.NewUserContext(l.ctx,
		openai.GetUserUniqueID(req.CustomerID, req.OpenKfID),
	).WithPrompt(l.basePrompt).WithModel(l.model).WithClient(c)

	//åˆ¤æ–­æ˜¯å¦éœ€è¦æ¸…é™¤èŠå¤©è®°å½•
	if clearContextTime > 0 {
		duration := time.Duration(clearContextTime) * time.Minute
		formattedTime := time.Now().Add(-duration).Format("2006-01-02 15:04:05")
		clearStatus, err1 := l.CheckClearContext(l.ctx, req.OpenKfID, req.CustomerID, formattedTime)
		if err1 != nil {
			util.Error("CustomerChatLogic:CustomerChat:è¯»å– stream å¤±è´¥:" + err1.Error())
			return nil, err1
		}
		if clearStatus {
			collection.Clear()
			collection.Messages = []openai.ChatModelMessage{}
			collection.Summary = []openai.ChatModelMessage{}
		}
	}

	// ç„¶å æŠŠ æ¶ˆæ¯ å‘ç»™ openai
	go func() {
		// åŸºäº summary è¿›è¡Œè¡¥å……
		messageText := ""
		if embeddingEnable {

			// md5 this req.MSG to key
			key := md5.New()
			_, _ = io.WriteString(key, req.Msg)
			keyStr := fmt.Sprintf("%x", key.Sum(nil))
			type EmbeddingCache struct {
				Embedding []float64 `json:"embedding"`
			}
			milvusService, err := milvus.InitMilvus(l.svcCtx.Config.Embeddings.Milvus.Host, l.svcCtx.Config.Embeddings.Milvus.Username, l.svcCtx.Config.Embeddings.Milvus.Password)
			if err != nil {
				fmt.Println(err.Error())
				return
			}
			defer milvusService.CloseClient()
			embeddingRes, err := redis.Rdb.Get(l.ctx, fmt.Sprintf(redis.EmbeddingsCacheKey, keyStr)).Result()
			var embedding []float64
			if err == nil {
				tmp := new(EmbeddingCache)
				_ = json.Unmarshal([]byte(embeddingRes), tmp)
				embedding = tmp.Embedding
			} else {
				res, err := c.CreateOpenAIEmbeddings(req.Msg)
				if err == nil {
					embedding = res.Data[0].Embedding
					// å»å°†å…¶å­˜å…¥ redis
					embeddingCache := EmbeddingCache{
						Embedding: embedding,
					}
					redisData, err := json.Marshal(embeddingCache)
					if err == nil {
						redis.Rdb.Set(l.ctx, fmt.Sprintf(redis.EmbeddingsCacheKey, keyStr), string(redisData), -1*time.Second)
					}
				}
			}

			if embeddingMode == "QA" {
				// å»é€šè¿‡ embeddings è¿›è¡Œæ•°æ®åŒ¹é…
				type EmbeddingData struct {
					Q string `json:"q"`
					A string `json:"a"`
				}
				var embeddingData []EmbeddingData
				result := milvusService.SearchFromQA(embedding, baseTopK)
				tempMessage := ""
				for _, qa := range result {
					if qa.Score > 0.3 {
						continue
					}
					if len(embeddingData) < 2 {
						embeddingData = append(embeddingData, EmbeddingData{
							Q: qa.Q,
							A: qa.A,
						})
					} else {
						tempMessage += qa.Q + "\n"
					}
				}
				if tempMessage != "" {
					go sendToUser(req.OpenKfID, "", req.CustomerID, "æ­£åœ¨æ€è€ƒä¸­ï¼Œä¹Ÿè®¸æ‚¨è¿˜æƒ³çŸ¥é“"+"\n\n"+tempMessage, l.svcCtx.Config)
				}
				for _, chat := range embeddingData {
					collection.Set(chat.Q, chat.A, false)
				}
				collection.Set(req.Msg, "", false)
			} else if embeddingMode == "ARTICLE" {
				//å¦‚æœæ˜¯articleæ¨¡å¼ï¼Œæ¸…ç†æ‰ä¸Šä¸‹åˆï¼Œå› ä¸ºæ–‡ç« å†…å®¹å¯èƒ½ä¼šå¾ˆé•¿
				collection.Clear()
				collection.Messages = []openai.ChatModelMessage{}
				collection.Summary = []openai.ChatModelMessage{}
				// å»é€šè¿‡ embeddings è¿›è¡Œæ•°æ®åŒ¹é…
				type EmbeddingData struct {
					text string `json:"text"`
				}
				var embeddingData []EmbeddingData
				result := milvusService.SearchFromArticle(embedding, baseTopK)

				for _, item := range result {
					fmt.Println("text:", item.CnText)
					fmt.Println("score:", item.Score)
					if item.Score < baseScore {
						continue
					}
					embeddingData = append(embeddingData, EmbeddingData{
						text: item.CnText,
					})
				}

				if len(embeddingData) > 0 {
					messageText += "When given CONTEXT you answer questions using only that information,and you always format your output in markdown.Answer with chinese.\n\n"
					messageText += "CONTEXT:"
					for _, chat := range embeddingData {
						messageText += chat.text + "\n\n"
						if c.WithModel(l.model).WithBaseHost(l.baseHost).GetNumTokens(messageText) > vars.MaxToken {
							break
						}
					}
					messageText += "USER QUESTION:" + req.Msg
					collection.Set(messageText, "", false)
				} else {
					collection.Set(req.Msg, "", false)
				}
			} else {
				collection.Set(req.Msg, "", false)
			}
		}

		collection.Set(req.Msg, "", false)

		prompts := collection.GetChatSummary()
		if l.svcCtx.Config.Response.Stream {
			channel := make(chan string, 100)
			go func() {
				defer close(channel)

				messageText1, err := c.ChatStream(prompts, channel)

				if err != nil {
					util.Error("CustomerChatLogic:CustomerChat:è¯»å– stream å¤±è´¥:" + err.Error())
					logx.Error("è¯»å– stream å¤±è´¥ï¼š", err.Error())
					sendToUser(req.OpenKfID, "", req.CustomerID, "ç³»ç»Ÿæ‹¥æŒ¤ï¼Œç¨åå†è¯•~"+err.Error(), l.svcCtx.Config)
					return
				}
				go func() {
					tokens := openai.NumTokensFromMessagesV2(prompts, messageText1, postModel)
					l.chatService.SaveTimesAndToken(req.CustomerID, tokens)
				}()

				collection.Set("", messageText1, true)
				// å†å»æ’å…¥æ•°æ®

				lastId, _ := l.chatService.InsertV2(&model.ChatRecord{
					RelationId:       questionModel.Id,
					User:             req.CustomerID,
					MessageId:        req.MsgID,
					OpenKfId:         req.OpenKfID,
					Content:          messageText1,
					Emoji:            0,
					ChatType:         repository.ChatTypeCustomer,
					AnswerOrQuestion: repository.AnswerOrQuestionAnswer,
					MessageType:      repository.MessageTypeText,
				})
				go service.NewRiskService(context.Background(), l.svcCtx).Reduce(req.CustomerID, req.OpenKfID, lastId)
				go l.wechatUserRepository.InsertWechatUserIfNotExist(req.CustomerID, true)
			}()

			var rs []rune
			// åŠ å¿«åˆæ¬¡å“åº”çš„æ—¶é—´ åç»­å¯æ”¹ä¸ºé˜¶æ¢¯å¼ï¼ˆç”¨æˆ·ä½“éªŒå¥½ï¼‰
			first := true
			for {
				s, ok := <-channel
				if !ok {
					// æ•°æ®æ¥å—å®Œæˆ
					if len(rs) > 0 {
						go sendToUser(req.OpenKfID, "", req.CustomerID,
							string(rs)+"\n--------------------------------\n"+req.Msg,
							l.svcCtx.Config,
						)
					}
					return
				}
				rs = append(rs, []rune(s)...)

				if first && len(rs) > 50 && strings.Contains(s, "\n\n") {
					go sendToUser(req.OpenKfID, "", req.CustomerID, strings.TrimRight(string(rs), "\n\n"), l.svcCtx.Config)
					rs = []rune{}
					first = false
				} else if len(rs) > 200 && strings.Contains(s, "\n\n") {
					go sendToUser(req.OpenKfID, "", req.CustomerID, strings.TrimRight(string(rs), "\n\n"), l.svcCtx.Config)
					rs = []rune{}
				}
			}
		}

		messageText, tokens, err := c.Chat(prompts)

		if err != nil {
			util.Error("CustomerChatLogic:CustomerChat:error:" + err.Error())
			sendToUser(req.OpenKfID, "", req.CustomerID, "ç³»ç»Ÿé”™è¯¯:"+err.Error(), l.svcCtx.Config)
			return
		}
		go l.chatService.SaveTimesAndToken(req.CustomerID, int64(tokens))
		// ç„¶åæŠŠæ•°æ® å‘ç»™å¯¹åº”çš„å®¢æˆ·
		go sendToUser(req.OpenKfID, "", req.CustomerID, messageText, l.svcCtx.Config)
		collection.Set("", messageText, true)

		lastId, _ := l.chatService.InsertV2(&model.ChatRecord{
			RelationId:       questionModel.Id,
			User:             req.CustomerID,
			MessageId:        req.MsgID,
			OpenKfId:         req.OpenKfID,
			Content:          messageText,
			Emoji:            0,
			ChatType:         repository.ChatTypeCustomer,
			AnswerOrQuestion: repository.AnswerOrQuestionAnswer,
			MessageType:      repository.MessageTypeText,
		})
		go service.NewRiskService(context.Background(), l.svcCtx).Reduce(req.CustomerID, req.OpenKfID, lastId)

		go l.wechatUserRepository.InsertWechatUserIfNotExist(req.CustomerID, true)

	}()

	return &types.CustomerChatReply{
		Message: "ok",
	}, nil
}

func (l *CustomerChatLogic) setModelNameV2() (ls *CustomerChatLogic) {
	m := l.svcCtx.Config.WeCom.Model
	if m == "" {
		m = openai.TextModel
	}
	l.svcCtx.Config.WeCom.Model = m
	l.model = m
	return l
}

func (l *CustomerChatLogic) CheckClearContext(ctx context.Context, openKfId, userId, formattedTime string) (bool, error) {

	chatPo, _, err := l.chatService.GetAllRecord(0, openKfId, userId, "", formattedTime, "", "", 0, 0, 0)
	if err != nil {
		return false, err
	}
	if len(chatPo) > 0 {
		return false, nil
	}
	return true, nil
}

// CustomerCommendSystem æŸ¥çœ‹ç³»ç»Ÿä¿¡æ¯
type CustomerCommendSystem struct{}

func (p CustomerCommendSystem) customerExec(l *CustomerChatLogic, req *types.CustomerChatReq) bool {
	tips := fmt.Sprintf(
		"ç³»ç»Ÿä¿¡æ¯\nç³»ç»Ÿç‰ˆæœ¬ä¸ºï¼š%s \nmodel ç‰ˆæœ¬ä¸ºï¼š%s \nç³»ç»ŸåŸºç¡€è®¾å®šï¼š%s \n",
		l.svcCtx.Config.SystemVersion,
		l.model,
		l.basePrompt,
	)
	sendToUser(req.OpenKfID, "", req.CustomerID, tips, l.svcCtx.Config)
	return false
}

// CustomerCommendHelp æŸ¥çœ‹æ‰€æœ‰æŒ‡ä»¤
type CustomerCommendHelp struct{}

func (p CustomerCommendHelp) customerExec(l *CustomerChatLogic, req *types.CustomerChatReq) bool {
	tips := fmt.Sprintf(
		"æ”¯æŒæŒ‡ä»¤ï¼š\n\n%s\n%s\n%s\n",
		"åŸºç¡€æ¨¡å—ğŸ•¹ï¸\n\n#help       æŸ¥çœ‹æ‰€æœ‰æŒ‡ä»¤",
		"#system æŸ¥çœ‹ä¼šè¯ç³»ç»Ÿä¿¡æ¯",
		"#clear æ¸…ç©ºå½“å‰ä¼šè¯çš„æ•°æ®",
	)
	sendToUser(req.OpenKfID, "", req.CustomerID, tips, l.svcCtx.Config)
	return false
}

type CustomerCommendAbout struct{}

func (p CustomerCommendAbout) customerExec(l *CustomerChatLogic, req *types.CustomerChatReq) bool {
	sendToUser(req.OpenKfID, "", req.CustomerID, "https://github.com/whyiyhw/chatgpt-wechat", l.svcCtx.Config)
	return false
}

type CustomerCommendDirect struct{}

func (p CustomerCommendDirect) customerExec(l *CustomerChatLogic, req *types.CustomerChatReq) bool {
	msg := strings.Replace(req.Msg, "#direct:", "", -1)
	sendToUser(req.OpenKfID, "", req.CustomerID, msg, l.svcCtx.Config)
	return false
}

func (l *CustomerChatLogic) setPostModel(postModel string) (ls *CustomerChatLogic) {
	var m string
	if "" != postModel {
		m = postModel
	}
	if m == "" {
		m = openai.ChatModel
	}
	l.postModel = m
	return l
}
